# Nginx configuration for LLM Chatbot System

events {
    worker_connections 1024;
}

http {
    # General settings for HTTP server
    server {
        listen 80;

        # Server Name
        server_name localhost;

        # Logging for debugging
        access_log /var/log/nginx/access.log;
        error_log /var/log/nginx/error.log;

        # FastAPI server reverse proxy
        location / {
            proxy_pass http://127.0.0.1:8001;  # Assuming FastAPI is running on port 8001
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            proxy_redirect off;
        }

        # Optionally, you can set a path to serve static files
        # location /static/ {
        #     root /path/to/static/files;
        # }

        # Ray Serve reverse proxy (if using multiple Ray Serve instances)
        location /vllm_service/ {
            proxy_pass http://127.0.0.1:8000;  # Assuming Ray Serve is running on port 8000
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            proxy_redirect off;
        }

        # Health Check endpoint (customize if necessary)
        location /healthcheck {
            proxy_pass http://127.0.0.1:8001/healthcheck;  # Assuming FastAPI has a health check endpoint
        }
    }

    # Optionally configure a default server for fallback
    server {
        listen 8080 default_server;
        server_name _;

        location / {
            return 404;
        }
    }
}
